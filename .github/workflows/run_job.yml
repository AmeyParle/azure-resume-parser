name: Run Image Batch Job
on: [workflow_dispatch]
jobs:
  run:
    runs-on: ubuntu-latest
    permissions: { id-token: write, contents: read }
    env:
      AZ_SUB: ${{ secrets.AZ_SUB }}
      AZ_RG:  ${{ vars.PREFIX }}-rg
      POOL_ID: imgpool
      JOB_ID: imgjob
      VM_SIZE: Standard_D2s_v3
      MAX_NODES: 5
      DEADLINE_S: 900
      MAX_CAP: 10
    steps:
      - uses: actions/checkout@v4
      - uses: azure/login@v2
        with: { client-id: ${{ secrets.AZ_CLIENT_ID }}, tenant-id: ${{ secrets.AZ_TENANT_ID }}, subscription-id: ${{ secrets.AZ_SUB }} }
      - name: Get TF outputs
        run: |
          pipx install azure-cli
          cd infra
          echo "STORAGE_URL=$(terraform output -raw storage_url)" >> $GITHUB_ENV
          echo "SAS=$(terraform output -raw sas_token)" >> $GITHUB_ENV
          echo "BATCH_ACC=$(terraform output -raw batch_name)" >> $GITHUB_ENV
      - name: Submit job
        run: |
          chmod +x scripts/submit_job.sh
          STORAGE_URL="$STORAGE_URL" SAS="$SAS" BATCH_ACC="$BATCH_ACC" \
          AZ_SUB="$AZ_SUB" AZ_RG="$AZ_RG" VM_SIZE="$VM_SIZE" MAX_NODES="$MAX_NODES" \
          ./scripts/submit_job.sh
      - name: Deadline watcher (auto-scale if needed)
        env:
          BATCH_ACC: ${{ env.BATCH_ACC }}
          JOB_ID:    imgjob
          POOL_ID:   imgpool
          DEADLINE_S: ${{ env.DEADLINE_S }}
          MAX_CAP:    ${{ env.MAX_CAP }}
        run: python3 scripts/watch_deadline.py
      - name: Download results
        run: |
          az storage blob download --container-name output --name final_report.txt --file final_report.txt --account-endpoint "$STORAGE_URL" --sas-token "$SAS"
          cat final_report.txt
